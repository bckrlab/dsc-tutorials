
@book{geron_hands-machine_2019,
	address = {Beijing [China] ; Sebastopol, CA},
	edition = {Second edition},
	title = {Hands-on machine learning with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: concepts, tools, and techniques to build intelligent systems},
	isbn = {978-1-4920-3264-9},
	shorttitle = {Hands-on machine learning with {Scikit}-{Learn}, {Keras}, and {TensorFlow}},
	publisher = {O'Reilly Media, Inc},
	author = {Géron, Aurélien},
	year = {2019},
	keywords = {Machine learning, Artificial intelligence, Python (Computer program language), TensorFlow},
}

@misc{noauthor_neural_nodate,
	title = {Neural {Networks} from {Scratch} in {Python} {Book}},
	url = {https://nnfs.io/},
	urldate = {2024-10-23},
	file = {Neural Networks from Scratch in Python Book:/home/bjarne/Zotero/storage/QFL8MTHN/nnfs.io.html:text/html},
}

@book{rashid_neuronale_2017,
	address = {Heidelberg},
	edition = {1. Auflage},
	title = {Neuronale {Netze} selbst programmieren: ein verständlicher {Einstieg} mit {Python}},
	isbn = {978-3-96009-043-4 978-3-96010-102-4 978-3-96010-103-1 978-3-96010-104-8},
	shorttitle = {Neuronale {Netze} selbst programmieren},
	abstract = {Neuronale Netze sind Schlüsselelemente des Deep Learning und der Künstlichen Intelligenz, die heute zu Erstaunlichem in der Lage sind. Sie sind Grundlage vieler Anwendungen im Alltag wie beispielsweise Spracherkennung, Gesichtserkennung auf Fotos oder die Umwandlung von Sprache in Text. Dennoch verstehen nur wenige, wie neuronale Netze tatsächlich funktionieren. Dieses Buch nimmt Sie mit auf eine unterhaltsame Reise, die mit ganz einfachen Ideen beginnt und Ihnen Schritt für Schritt zeigt, wie neuronale Netze arbeiten: - Zunächst lernen Sie die mathematischen Konzepte kennen, die den neuronalen Netzen zugrunde liegen. Dafür brauchen Sie keine tieferen Mathematikkenntnisse, denn alle mathematischen Ideen werden behutsam und mit vielen Illustrationen und Beispielen erläutert. Eine Kurzeinführung in die Analysis unterstützt Sie dabei. - Dann geht es in die Praxis: Nach einer Einführung in die populäre und leicht zu lernende Programmiersprache Python bauen Sie allmählich Ihr eigenes neuronales Netz mit Python auf. Sie bringen ihm bei, handgeschriebene Zahlen zu erkennen, bis es eine Performance wie ein professionell entwickeltes Netz erreicht. - Im nächsten Schritt tunen Sie die Leistung Ihres neuronalen Netzes so weit, dass es eine Zahlenerkennung von 98 \% erreicht – nur mit einfachen Ideen und simplem Code. Sie testen das Netz mit Ihrer eigenen Handschrift und werfen noch einen Blick in das mysteriöse Innere eines neuronalen Netzes. - Zum Schluss lassen Sie das neuronale Netz auf einem Raspberry Pi Zero laufen. Tariq Rashid erklärt diese schwierige Materie außergewöhnlich klar und verständlich, dadurch werden neuronale Netze für jeden Interessierten zugänglich und praktisch nachvollziehbar},
	language = {ger},
	publisher = {O'Reilly},
	author = {Rashid, Tariq},
	translator = {Langenau, Frank},
	year = {2017},
}

@book{rashid_make_2016,
	address = {s.l.},
	title = {Make {Your} own neural network},
	isbn = {978-1-5308-2660-5},
	language = {eng},
	publisher = {CreateSpace Independent Publishing Platform},
	author = {Rashid, Tariq},
	year = {2016},
}

@book{russell_artificial_2022,
	address = {Boston},
	edition = {Fourth edition, global edition},
	series = {Prentice {Hall} series in artificial intelligence},
	title = {Artificial intelligence: a modern approach},
	isbn = {978-1-292-40113-3 978-1-292-40117-1},
	shorttitle = {Artificial intelligence},
	abstract = {The most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence The long-anticipated revision of Artificial Intelligence: A Modern Approach explores the full breadth and depth of the field of artificial intelligence (AI). The 4th Edition brings readers up to date on the latest technologies, present concepts in a more unified manner, and offers new or expanded coverage of machine learning, deep learning, transfer learning, multi agent systems, robotics, natural language processing, causality, probabilistic programming, privacy, fairness, and safe AI},
	language = {eng},
	publisher = {Pearson},
	author = {Russell, Stuart J. and Norvig, Peter},
	collaborator = {Chang, Ming-wei and Devlin, Jacob and Dragan, Anca and Forsyth, David and Goodfellow, Ian and Malik, Jitendra and Mansinghka, Vikash and Pearl, Judea and Wooldridge, Michael J.},
	year = {2022},
}

@misc{noauthor_norvigpytudes_nodate,
	title = {norvig/pytudes: {Python} programs, usually short, of considerable difficulty, to perfect particular skills.},
	url = {https://github.com/norvig/pytudes/tree/main},
	urldate = {2024-10-23},
	file = {norvig/pytudes\: Python programs, usually short, of considerable difficulty, to perfect particular skills.:/home/bjarne/Zotero/storage/RSPWEJP7/main.html:text/html},
}

@article{silverman_e_1989,
	title = {E. {Fix} and {J}.{L}. {Hodges} (1951): {An} {Important} {Contribution} to {Nonparametric} {Discriminant} {Analysis} and {Density} {Estimation}: {Commentary} on {Fix} and {Hodges} (1951)},
	volume = {57},
	issn = {0306-7734},
	shorttitle = {E. {Fix} and {J}.{L}. {Hodges} (1951)},
	url = {https://www.jstor.org/stable/1403796},
	doi = {10.2307/1403796},
	abstract = {In 1951, Evelyn Fix and J.L. Hodges, Jr. wrote a technical report which contained prophetic work on nonparametric discriminant analysis and probability density estimation, and which was never published by the authors. The report introduced several important concepts for the first time. It is of interest not only for historical reasons but also because it contains much material that is still of contemporary relevance. Here, the report is printed in full together with a commentary placing the paper in context and interpreting its ideas in the light of more modern developments. /// En 1951, E. Fix et J.L. Hodges, Jr. ont écrit un rapport technique prophétique sur l'analyse non-paramétrique de discrimination et l'estimation de la densité de probabilité, mais celui-ci ne fut jamais publié par ses auteurs. Ce rapport introduit plusieurs idées nouvelles et importantes. Il nous intéresse non seulement pour des raisons historiques, mais aussi parce qu'il contient des concepts qui sont encore importants de nos jours. Nous le publions ici en entier, accompagné d'un commentaire qui l'interprète d'un point de vue plus moderne.},
	number = {3},
	urldate = {2024-11-14},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Silverman, B. W. and Jones, M. C.},
	year = {1989},
	note = {Publisher: [Wiley, International Statistical Institute (ISI)]},
	pages = {233--238},
	file = {JSTOR Full Text PDF:/home/bjarne/Zotero/storage/NAAF73YH/Silverman and Jones - 1989 - E. Fix and J.L. Hodges (1951) An Important Contribution to Nonparametric Discriminant Analysis and.pdf:application/pdf},
}

@article{fix_discriminatory_1989,
	title = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}: {Consistency} {Properties}},
	volume = {57},
	issn = {0306-7734},
	shorttitle = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}},
	url = {https://www.jstor.org/stable/1403797},
	doi = {10.2307/1403797},
	number = {3},
	urldate = {2024-11-14},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Fix, Evelyn and Hodges, J. L.},
	year = {1989},
	note = {Publisher: [Wiley, International Statistical Institute (ISI)]},
	pages = {238--247},
	file = {JSTOR Full Text PDF:/home/bjarne/Zotero/storage/C7HRB7NB/Fix and Hodges - 1989 - Discriminatory Analysis. Nonparametric Discrimination Consistency Properties.pdf:application/pdf},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
	language = {en},
	urldate = {2025-01-22},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:/home/bjarne/Zotero/storage/7Q5MWQEW/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2025-05-27},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/bjarne/Zotero/storage/KLSHMS93/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf;Snapshot:/home/bjarne/Zotero/storage/PJ5H95WU/1312.html:text/html},
}
