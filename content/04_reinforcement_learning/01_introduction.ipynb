{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279a04d0",
   "metadata": {},
   "source": [
    "# An Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8db61",
   "metadata": {},
   "source": [
    "> A brief introduction to the main concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742cbdf",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a type of Machine Learning, where agents learn to make decisions in a dynamic environment to maximize a reward.\n",
    "\n",
    "The typical Reinforcement Learning loop looks like this:\n",
    "\n",
    "1. The agent receives an *observation*, representing the current state of the environment.\n",
    "2. Based on the observation and its current *policy*, the agent selects an *action*.\n",
    "3. The agent performs the action and receives a *reward* based on the environment's new state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce6beb",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3c9d3",
   "metadata": {},
   "source": [
    "Formally, RL can be understood as solving *Markov Decision Process* (MDP). A MDP is a $5$-tuple $(S, A, T, \\pi_0, R)$, with:\n",
    "\n",
    "- $S$: state space of the environment\n",
    "- $A$: action space of the environment, i.e., actions that can be performed by the agent\n",
    "- $T : S \\times A \\times S \\mapsto [0,1]$: transition function, that describes how actions affect the state of the environment\n",
    "- $r: S \\times A \\times S \\mapsto \\mathbb{R}$: reward function\n",
    "- $\\pi_0: S \\mapsto [0,1]$: probability distribution over initial state\n",
    "\n",
    "The agent selects actions using a *policy* $\\pi: S \\mapsto A$.\n",
    "The objective is usually to find the optimal policy $\\pi^*$, which maximizes the cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ce3e9",
   "metadata": {},
   "source": [
    "> **Questions**\n",
    ">\n",
    "> 1. Why does the cartesian product that defines the domain of the transition function include the state space $S$ twice?\n",
    "> 2. Why does the transition function return a value between 0 and 1?\n",
    "> 3. Assume that from the current state S, you can get to state A with an immediate high reward, or state B with an immediate low reward. Is state A always prefarable over B?\n",
    "> 4. Can teaching a dog a new trick be understood as a Markov Decision Process? If yes, what are the state space, action space, and the reward function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e5e3c6",
   "metadata": {},
   "source": [
    "## Bookmarks\n",
    "\n",
    "- [Kaggle Intro to Reinforcement Learning](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320fd66f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
