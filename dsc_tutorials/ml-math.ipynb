{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundations for Machine Learning\n",
    "\n",
    "> This notebook provides a brief and basic (!) introduction to mathematical concepts frequently encountered in Machine Learning.\n",
    ">\n",
    "> **Author:** Bjarne C. Hiller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Probability of $A$ given $B$:\n",
    "\n",
    "$$\n",
    "    p(A | B) = \\frac{p(A, B)}{p(B)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' Law:\n",
    "\n",
    "$$\n",
    "    \\underbrace{p(y|x)}_{\\text{posterior}} = \\frac{ \\overbrace{p(x|y)}^{\\text{likelihood}} \\overbrace{p(y)}^{\\text{prior}} }{ \\underbrace{p(x)}_{\\text{evidence}} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain Rule for conditional Probabilities:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        p(x_1, x_2, x_3, ..., x_n) = & \\Pi_{i=1}^n p(x_i | x_{i+1}, ..., x_n) \\\\\n",
    "        = & p(x_1 | x_2, x_3, ..., x_n) \\\\\n",
    "        \\cdot & p(x_2 | x_3, ..., x_n) \\\\\n",
    "        \\cdot & ... \\\\\n",
    "        \\cdot & p(x_{n-1} | x_n) \\\\\n",
    "        \\cdot & p(x_n)\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p$-norm (*Minkowski norm*)\n",
    "\n",
    "$$\n",
    "    \\lVert \\mathbf{x} \\rVert_p = \\left( \\sum_{i=1}^d x_i^p \\right)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "For $p=1$, we just add up the individual vector components. Therefore, the norm is known as *Taxicab norm* or *Manhatten norm*, since distances can be visualized by a taxi, that can just drive on orthogonal streets between buildings to get to its destination:\n",
    "\n",
    "$$\n",
    "    \\lVert \\mathbf{x} \\rVert_1 = \\sum_{i=1}^d \\lvert x_i \\rvert\n",
    "$$\n",
    "\n",
    "For $p=2$, we get the well-known *Euclidean Norm*, that returns the distance of a point to the origin:\n",
    "\n",
    "$$\n",
    "    \\lVert \\mathbf{x} \\rVert_2 = \\sqrt{\\sum_{i=1}^d x_i^2 } = \\sqrt{\\mathbf{x} \\cdot \\mathbf{x}}\n",
    "$$\n",
    "\n",
    "Using the Euclidean norm, we get the Euclidean Distance, which describes the length of a line segment connecting the 2 points:\n",
    "\n",
    "$$\n",
    "    d(\\mathbf{x}, \\mathbf{y}) = \\lVert \\mathbf{x} - \\mathbf{y} \\rVert_2 = \\sqrt{\\sum_{i=0}^d (x_i - y_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What do we get for $p=\\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the *maximum norm*:\n",
    "\n",
    "$$\n",
    "    \\lVert \\mathbf{x} \\rVert_\\infty = \\max_i x_i\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
